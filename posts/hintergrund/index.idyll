[meta title:"Hintergrund ‚Äì Kommentare.vis.one" description:"Mehr Informationen zu Daten & Verfahren" url:"https://kommentare.vis.one/hintergrund/" twitterHandle:"@fil_ter" shareImageUrl:"https://kommentare.vis.one/static/images/preview.png" /]

[Header
  fullWidth:true
  title:"Hintergrund Kommentare.vis.one"
  subtitle:"Mehr Informationen zu Daten & Verfahren"
  background:"#222222"
  color:"#ffffff"
   /]

Auf dieser Seite, dem dritten Teil, werden weitergehende Informationen aufgelistet und offene Fragen beantwortet. Bei weiteren Fragen bitte ein E-Mail an [link text:"hi@jfilter.de" url:"mailto:hi@jfilter.de" /] schreiben. 

[Im ersten Teil des Online-Projekts](/) gibt eine generelle Einf√ºhrung zum Machine Learning und [im zweiten Teil](/zeit/) geht um √Ñnderung von Sprache √ºber die Zeit.

### Wie genau werden mithilfe von Word Embeddings Kommentare klassifiziert?

Word Embeddings sind dazu da, W√∂rter in Zahlen zu √ºberf√ºhren. Das ist notwendig, damit "die Zahlen" in ein Modell √ºberf√ºhrt werden und trainiert werden k√∂nnen. Das Modell besteht aus Parameter, die angepasst werden k√∂nnen. In einer Trainingsphase werden dem Modell Kommentare pr√§sentiert und das Modell muss raten. Ist es Hasskommentar (mit Beleidigung) oder nicht? Wenn das Modell richtig liegt, passiert nichts. Wenn es falsch liegt, dann werden automatisiert die Parameter angepasst.

In der Folge hat man ein Modell, das man gezielt einsetzen kann. In der Theorie soll es dann in der Lage sein, auch bei neuen und bislang unbekannten Daten zwischen Beleidigungen und nicht-Beleidigungen unterscheiden zu k√∂nnen. 

[SVG src:"/hintergrund/static/ml.svg" /]
*Der Kommentar links unten (rot) wird in das Modell gef√ºttert. Wenn das Modell den Kommentar so wie Menschen bewertet, passiert nichts. Mach das Modell einen Fehler, wird es angepasst.*

### Was sind Word Embeddings?

Word Embedding stehen f√ºr Verfahren, bei denen W√∂rtern ein Vektor zugewiesen wird. Ein Vektor ist eine feste Anzahl (z. B. 300 oder 500) von Werten zwischen -1 und 1. In diesem Vektorraum sind √§hnliche W√∂rter enger zusammen. Mit Word Embeddings werden aus W√∂rter Zahlen. √Ñhnliche W√∂rter haben idealerweise auch √§hnliche Vektoren.

### Wie genau wurden die Word Embeddings erstellt?

Es gibt unterschiedliche Verfahren wie Word Embeddings erstellt werden, doch das Grundprinzip ist das Gleiche. Basierend auf einer gro√üen Menge an Text wird automatisch analysiert, welche W√∂rtern mit welchen anderen W√∂rtern zusammen vorkommen, sogenannte Kollokation. [Die S√ºddeutsche Zeitung erkl√§rt Word Embeddings in einem aktuellen Artikel ausf√ºhrlich.](https://projekte.sueddeutsche.de/artikel/politik/so-haben-wir-den-bundestag-ausgerechnet-e893391/)

Aber im Gegensatz zu dem Ansatz der S√ºddeutsche Zeitung wurden f√ºr diese Analyse ein leicht anderes Verfahren verwendet (PPMI/SVD statt Word2Vec). F√ºr Interessierte gibt ein [wissenschaftliches Paper Hintergrundinformationen](https://www.aclweb.org/anthology/Q15-1016/).

### Wie wird die √Ñhnlichkeit zwischen W√∂rtern errechnet?

Da die W√∂rter aus Vektoren (eine Reihe von Zahlen (z. B. 500)) bestehen, dient der Winkel zwischen zwei Vektoren als Ma√ü der √Ñhnlichkeit. Zum Beispiel mithilfe der [Kosinus-√Ñhnlichkeit](https://de.wikipedia.org/wiki/Kosinus-%C3%84hnlichkeit) wird dies errechnet.

### Wie kann ein Word Embedding auf einer Ebene dargestellt werden?

Um die Begriffe auf einer Ebene zubringen, brauch man Vektoren die aus zwei Dimensionen bestehen (f√ºr die x-Ache und y-Achse). Dazu benutzen wird das Verfahren der Dimensionsreduktion. Dieses "komprimiert" die Daten so, dass m√∂glichst wenige Informationen verloren werden. Es wird [Hauptkomponentenanalyse (Principal Component Analyse (PCA))](https://de.wikipedia.org/wiki/Hauptkomponentenanalyse) verwendet.

Damit man sinnvolle Ergebnisse erh√§lt, kann man nur auf einen Ausschnitt der Daten machen. Alle (z. B. 100.000) W√∂rter k√∂nnen sowieso nicht auf einmal dargestellt werden. Zudem funktioniert die Kompression so besser, auf desto weniger W√∂rtern gearbeitet wird. Daher werden z. B. zu einem gegebenen Begriff nur z. B. 10 W√∂rter dargestellt.

### Woher kommen die Daten?

Die Kommentare stammen aus der Kommentarspalte von Zeit Online und wurden zwischen 2010 bis Sommer 2019 erstellt. Insgesamt gibt es √ºber 13 Millionen Kommentare bestehend aus rund 45 Millionen S√§tze. Die S√§tze verteilen sich folgenderma√üen √ºber die Zeit:

2010-2011:  ~3 Millionen S√§tze

2012-2013:  ~4.5 Millionen S√§tze

2014-2015:  ~8 Millionen S√§tze

2016-2017:  ~14 Millionen S√§tze

2018-2019:  ~15 Millionen S√§tze

Zeit Online ist eine der wenigen Zeitungen, die noch Kommentare unter ihren Artikeln zulassen.

### Wie wurde der Text der Kommentare bearbeitet?

Zun√§chst wurden W√∂rter ohne Bedeutung, sogenannte ‚ÄúStop Words‚Äù, wie z. B. Artikel (der, die das) entfernt. Zudem wurden alle W√∂rter auf ihren Wortstamm zur√ºckgef√ºhrt (‚Äúlemmatized‚Äù). Also z. B. das Plural-S entfernt bei Verben auf den Infinitiv ersetzt. Hier kommt es durch die automatisierte Verarbeitung manchmal zu Fehlern. Zudem wurden alle W√∂rter kleingeschrieben und Ziffern durch 0 ersetzt. Da im Deutschen W√∂rter in unterschiedliche Formen vorkommen (z. B. Konjugation), wurden somit die Anzahl an unterschiedlichen W√∂rter reduziert, um bessere Ergebnisse auf kleinen Textkorpora zu erzielen.

### Kann ich die Word Embeddings haben?

Ja, sie stehen zum Download bereit. Es gibt ein Embedding f√ºr alle Kommentare. Und 5 F√ºr die zwei-j√§hrigen Zeitepochen. Sie stehen unter [CC-0-Lizenz](https://creativecommons.org/share-your-work/public-domain/cc0/), k√∂nnen also ohne Namensnennung verwendet werden.

üëâ https://data.jfilter.de/nlp/kommentare/

Die Word Embeddings wurden mit der eigens entwickelten Bibliothek [hyperhyper](https://github.com/jfilter/hyperhyper) entwickelt. 

Folgende Parameter wurden f√ºr das Embedding mit allen Kommentaren (2010 - 2019) verwendet: 

```
window: 10, dynamic_window: 'decay', decay_rate: 0.375, delete_oov: true, subsample: 'deter', subsample_factor: 7.268e-05, neg: 1.5, eig: 0.3, dim: 500
```

Und diese f√ºr die f√ºnf anderen Embeddings f√ºr je zwei Jahre

```
window: 10, dynamic_window: 'decay', decay_rate: 0.33, delete_oov: true, subsample: 'deter', subsample_factor: 7e-05, neg: 1, eig: 0.35, dim: 500
```

Die Hyperparameter wurden durch Experimente auf [Evaluationsdaten](https://github.com/jfilter/hyperhyper/tree/master/hyperhyper/evaluation_datasets/de) herausgefunden.

### Warum hast du eine eigene Software-Bibliothek entwickelt?

F√ºr die bekannten Verfahren [Word2Vec](https://code.google.com/archive/p/word2vec/) von Google und [fastText](https://github.com/facebookresearch/fastText) von Facebook gibt es effiziente Implementierungen. Aber die Methoden orientieren sich an den Bed√ºrfnissen gro√üer Tech-Unternehmen, diese sind im Besitz sehr gro√üer Datenmengen. Das ist nicht immer der Fall, wie hier. 

F√ºr alternative Verfahren, f√ºr die es weniger Daten brauch, gab es bereits ein [wissenschaftliches Paper](https://www.aclweb.org/anthology/Q15-1016/), aber keine effiziente Implementierung. [Das habe ich ge√§ndert](https://github.com/jfilter/hyperhyper).

### Kann ich die Software benutzen?

Ja, s√§mtliche verwendete und programmierte Software steht unter Open-Source-Lizenz.

Hier eine √úbersicht √ºber den verwendeten Code und eine detaillierte Anleitung: https://github.com/jfilter/ptf-kommentare

[hr /]

Bei offenen Fragen bitte ein E-Mail an [link text:"hi@jfilter.de" url:"mailto:hi@jfilter.de" /] schreiben.

[hr /]


## √úbersicht

* [Teil I: Einf√ºhrung](/)
* [Teil II: Ver√§nderung in den Kommentaren](/zeit/)
* *[Teil III: Hintergrund zu Daten & Verfahren](/hintergrund/)*

[small]
[Vortrag beim 95. Netzpolitischen Abend der Digitalen Gesellschaft in der c-base: Warum automatisierte Filter rassistisch sind](https://www.youtube.com/watch?v=yQKGiootIfs)
[/small]
[br][/br]
[br][/br]
[small]
[Gastbeitrag bei Netzpolitik.org: Warum automatisierte Filter rassistisch sind](https://netzpolitik.org/2020/warum-automatisierte-filter-rassistisch-sind/)
[/small]


[hr /]

[Thanks/]