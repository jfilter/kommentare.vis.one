[meta title:"Hintergrund ‚Äì Kommentare.vis.one" description:"Maschinelles Lernen wird als L√∂sung verkauft, um Hass aus dem Internet zu filtern. Diese Webseite erkl√§rt wie 'die Maschinen' die Bedeutung von W√∂rtern erlernen." url:"https://kommentare.vis.one/hintergrund" twitterHandle:"@fil_ter" shareImageUrl:"https://kommentare.vis.one/static/preview.png" /]

[Header
  fullWidth:true
  title:"Hintergrund zu Daten & Verfahren"
  subtitle:"Hier werden alle Fragen beantwortet"
  background:"#222222"
  color:"#ffffff"
   /]

## Hintergrundinformationen zu Kommentare.vis.one

Auf dieser Seite werden weitergehende Informationen aufgelistet. Und Fragen beantwortet. Bei offenen Fragen bitte ein E-Mail an [link text:"hi@jfilter.de" url:"mailto:hi@jfilter.de" /] schreiben. 

### Was sind Word Embeddings?

Word Embedding stehen f√ºr Verfahren, bei denen W√∂rtern ein Vektor zugewiesen wird. Ein Vektor ist eine feste Anzahl (z. B. 300) von Werten zwischen -1 und 1. In diesem Vektorraum sind √§hnliche W√∂rter enger zusammen. Mit Word Embeddings werden aus W√∂rter Zahlen.

### Wie genau wurden die Word Embeddings erstellt?

Es gibt unterschiedliche Verfahren wie Word Embeddings erstellt werden, doch das Grundprinzip ist das Gleiche. Basierend auf einer gro√üen Menge an Text wird automatisch analysiery, werde W√∂rtern mit welchen anderen W√∂rter vorkommt. Das wird dadurch erreicht, indem man √ºber alle S√§tze iterirt. In einer gewissen Fenstegr√∂√üer werden Koolokatione vermerkt.

[Die S√ºddeutsche Zeitung erkl√§rt Word Embeddings f√ºr ihne Auswertung ausf√ºhrlich.](https://projekte.sueddeutsche.de/artikel/politik/so-haben-wir-den-bundestag-ausgerechnet-e893391/)

Im Gegensatz zu der der S√ºddeutsche Zeitung wurden f√ºr diese Analyse ein Verfahrn verwendet, welches Wordpaare z√§hlt (PPMI/SVD). F√ºr Interessierte gibt ein [wissenschaftliche Paper Hintergrundinformationen](https://www.aclweb.org/anthology/Q15-1016/).

### Bilden Word Embeddings die Realit√§t ab?

Nein, erst einmal bilden Word Embeddings nur die Sprache in den Trainingsdaten ab. Die Sprache in den Kommentaren steht nichts zwangsl√§ufig w√ºrde die Wahrheit oder Realit√§t ab. 

Ein Problem ist, dass bei gro√üer Textmenge dann auch zum Bias kommen kann. So hat ein Forscher herausgefunden, dass eine Google-API, die zu √§hnliches bentuzen. Das Word Mexiso negativ eingeordetnet. Das kam dadurch dass auf Hasskommentaren traniert wurd. Und es gab viel mehr negative Kommentem zu Mexio als zu anderern L√§ndern.

### Wie wird die √Ñhnlichkeit zwischen W√∂rtern errechnet?

Da die W√∂rter aus Vektoren (eine Reihe von Zahlen (z. B. 500)) bestehen, dient der Winkel als Zeichen der √Ñhnlichkeit. Mithilfe des [Kosinus-√Ñhnlichkeit](https://de.wikipedia.org/wiki/Kosinus-%C3%84hnlichkeit) wird diese errechnet.

### Wie kann die √§hnliches Begriffe auf einer Ebene dargestellt werden?

Um die Begriffe auf einer Ebene zubringen, brauch man Vektoren die aus zwei Dimensionen bestehen (f√ºr die X-Ache und Y-Achse). Dazu benutzen wird das Verfahen der Dimeionsreduktion. Dieses "komprimiert" die Daten so, dass m√∂glichst wenige Informationen verloren werden. Es wird [Hauptkomponentenanalyse (Principal Component Analyse (PCA))](https://de.wikipedia.org/wiki/Hauptkomponentenanalyse) verwendet.

### Woher kommen die Daten?

Die Kommentare stammen aus deutschen Kommentarspalten von 2010 bis Sommer 2019. Ingesamt gibt es √ºber 13 Millionen Kommentare bestehend aus rund 45 Millionen S√§tze. Die S√§tze verteilen sich auf folgenderma√üen √ºber die Zeit:

2010-2011:  ~3 Millionen S√§tze

2012-2013:  ~4.5 Millionen S√§tze

2014-2015:  ~8 Millionen S√§tze

2016-2017:  ~14 Millionen S√§tze

2018-2019:  ~15 Millionen S√§tze


### Wie wurde der Text bearbeitet?

Zun√§chste wurden W√∂rter ohne Bedeutung, sogennante "Stop Words", wie z. B. Artikel entfernt. Zudem wurden alle W√∂rter auf ihren Wortstamm zur√ºckgef√ºhrt ("lemmatized"). Also z. B. das Plural-S entfernt bei Verben auf den Infinitiv ersetzt. Hier kommt es durch die automatiserte Verabeitung manchmal zu Fehlern. Zudem wurden alle W√∂rter klein geschrieben und Ziffern durch 0 ersetzt. Da im Deutschen W√∂rter in unterschiedliche Formen vorkommen (z. B. Konjugation), wurden somit die Anzahl an unterschiedlichen W√∂rter reduziert, um bessere Ergebnisse auf kleinen Text-Corpora zu erzielen.

### Kann ich die Word Embeddings haben?

Ja, sie stehen zum Download bereit. Es gibt ein Embedding f√ºr alle Kommentare. Und 5 F√ºr die zwei-j√§hrigen Zeitepochen. Sie stehen unter [CC-0-Lizenz](https://creativecommons.org/share-your-work/public-domain/cc0/), k√∂nnen also ohne Namensnennung verwendet werden.

üëâ https://data.jfilter.de/nlp/kommentare/

Die Word Embeddings wurden mit der eigens entwickelten Bibliothek [hyperhyper](https://github.com/jfilter/hyperhyper) entwickelt. 

Folgende Parameter wurden f√ºr das Embedding mit allen Kommentaren verwendet: 

```
window: 10, dynamic_window: 'decay', decay_rate: 0.375, delete_oov: true, subsample: 'deter', subsample_factor: 7.268e-05, neg: 1.5, eig: 0.3, dim: 500
```

Und diese 

```
window: 10, dynamic_window: 'decay', decay_rate: 0.33, delete_oov: true, subsample: 'deter', subsample_factor: 7e-05, neg: 1, eig: 0.35, dim: 500
```

Die Hyperparameter wurden durch Experimente auf [Evaluationsdaten](https://github.com/jfilter/hyperhyper/tree/master/hyperhyper/evaluation_datasets/de) herausgefunden.

### Kann ich die Software benutzen?

Ja, s√§mtliche verwendete und programmierte Software steht unter Open-Source-Lizenz.

Hier eine √úbersicht √ºber den verwendeten Code: https://github.com/jfilter/ptf

[hr /]

Bei offenen Fragen bitte ein E-Mail an [link text:"hi@jfilter.de" url:"mailto:hi@jfilter.de" /] schreiben.

[hr /]


## √úbersicht

* [Teil I: Einf√ºhrung](/)
* [Teil II: Ver√§nderung in den Kommentaren](/zeit/)
* **[Teil III: Hintergrund zu Daten & Verfahren](/hintergrund/)**

[hr /]

[Thanks/]