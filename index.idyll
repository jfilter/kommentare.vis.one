[meta title:"Einführung – Kommentare.vis.one" description:"NLP am Beispiel von Online-Kommentaren erklärt" /]


[FilterHeader
  subtitle:"Maschinelles Lernen wird als Lösung verkauft, um Hass aus dem Internet zu filtern. Diese Webseite erklärt wie 'die Maschienen' die Bedeutung von Wörtern erlernen."
  fullWidth:true
  author:"Johannes Filter"
  authorLink:"https://johannesfilter.com"
  date:"15. Februar 2020"
  background:"#222222"
  color:"#ffffff"
/]

[MovingTexts /]

// Zudem haben wir 15 Millionen Kommentare analysiert, wie sich die Sprache über die letzten zehn Jahre in der Kommentarspalte verändert hat.

// https://github.com/FormidableLabs/victory/blob/master/packages/victory-core/src/victory-theme/grayscale.js

[derived name:'scatter-theme' value:`{
  axis: {
    style: {
      axis: {
        fill: 'transparent',
        stroke: 'transparent'
      },
      grid: {
        stroke: 'transparent',
      },
      tickLabels: {
        stroke: 'transparent',
        fill: 'transparent'
      }
    }
  },
  scatter: {
    style: {
      data: {
        fill: "black",
        cursor: 'pointer',
        opacity: 0.8
      },
      labels: {
        fontFamily: 'lato',
        fontSize: () => Math.max(window.screen.width, window.innerWidth) < 768 ? 20 :  8,
        letterSpacing: 'normal',
        padding: 3,
        fill: 'charcoal',
        stroke: "transparent"
      }
    }
  }
}` /]

In den Kommentarspalten tobt der Hass. Um diesen einzudämmen, prüfen Zeitungsredaktionen manuell die Kommentare. Doch aufgrund finanzieller Engpässe, schließen viele Zeitungen die Kommentarfunktion gleich ganz ab. In diesem Kontext gelobt die automatisierte Kommentareauswertung Besserung. Jedoch ist aktuell noch unklar, welche Aufgaben wir den Maschinen überlassen wollen. Damit es darüber eine gesellschaftliche Debatte gibt, müssen zuerst breite Teile der Bevölkerung die dahinter liegenden technischen Mechanismen verstehen. Mit dieser Webseite werden einzelne Verfahren des Machinelles Lernens am Beispiel von Kommentaren erklärt. Der Fokus liegt auf der Verarbeitung von Sprachen, Computerlinguistik (oder Verarbeitung natürlicher Sprache (engl. *Natural-Language Processing* )) funktioniert.

![](/static/images/news.png)
*[CC-BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/), [Christoph Hoppenbrock](https://bildbauer.de/), verändert ([Original](https://demokratielabore.de/workshops/newsbusters/))*

###### ignore please

## Was ist Maschinelles Lernen?

Mit dem Aufkommen der Informationstechnolgie im 20. Jahrhundert wurden immer mehr Entscheidungen an Computer abgegeben. Aufgaben wurden mithilfe von Programm-Code gelöst. Programmier:innen formulieren einen sogennanten Algorithmus, welche eine Schritt-für-Schritt-Rezept darstellt. Ähnlich zu einem Kochrezept werden Schritte festgelegt. Wie könnte ein Algorithmus aussehen, der Hasskommentare finden sollte? Ein Ansatz:

1. Man schreibt eine lange Liste von Beleidigungen auf.
2. Wenn eine Beleidigung in einem Kommentar vorkommt, wird der Kommentar aussortiert.

Nur lässt sich der Ansatz leicht austricksen, in dem man immer neue Wörter kreiert. Außderm würde die Länge der Liste ausufern. 

In diesem Zusammenhang gibt es eine neue Art Probleme zu Lösungen. Man füttert den Computer mit Daten und der Computer soll selbst lernen, welche Wörter als Beleidung zählt. Ansätze dieser Art nennt man Maschinelles Lernen (engl. *Machine Learning*). Es gibt zwei Arten von Maschine Learning: 
1. Trainingsdaten werden manuell annotiert und die Maschiene soll lernen von diesen zu extrapolieren
2. Es werden keine Daten extra annotiert, sondern von Information auf alten geschlossen werden.

Wir fokussieren auf den 2. Ansatz. Aus einer großen Menge an Text soll der Computer Wörter erlernen.

## Sprache, Computer und Representation

Auf der untersten Ebene werden Daten als 0 oder 1 abgespeichert. Es gibt jedoch Abstraktionseben die darauf aufbauen. Aus 0 und 1 werden Zeichen. Und aus Zeichen werde Buchstaben. Mit Buchstaben kann man Wörter und damit ganze Texte bauen. Für unseren Fall wollen wir einen Text als eine Liste von Wörtern betrachten. Wie kommen wir dann von Wörtern auf Bedeutung? 

![](/static/images/computer.png)
*[CC-BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/), [Christoph Hoppenbrock](https://bildbauer.de/), verändert ([Original](https://demokratielabore.de))*

###### bla


## Bedeutung durch Kontext

Die Grundlegende Ansatz: Die Kolokation von Wörtern. Also welche Wörter werden mit welchen anderen Wörter im gleichen Satz auf. Der englische Linguister [John Rupert Firth](https://en.wikipedia.org/wiki/John_Rupert_Firth) stellt Mitte des 20. Jahrhunerts einige Theoreme auf.

[Aside]
> You shall know a word by the company it keeps. 
[/Aside]

> Du sollst ein Wort von der Begleitung kennen, die sie führt. 

[Aside]
> The complete meaning of a word is always contextual, and no study of meaning apart from context can be taken seriously. 
[/Aside]

> Die vollständige Bedeutung eines Wortes ist immer kontextabhängig, und keine Studie über die Bedeutung außer dem Kontext kann ernst genommen werden. 

Der Kontext entscheidet über die wahre Beudeutung eines Wortes. Und ein Wort kann durch 

[Aside]
> Meaning by collocation is an abstraction at the syntagmatic level and is not directly concerned with the conceptual or idea approach to the meaning of words. One of the meanings of _night_ is its collocability with _dark_, and of _dark_, of course, collocation with _night_.
[/Aside]

> Die Bedeutung durch Kollokation ist eine Abstraktion auf der syntagmatischen Ebene und hat nichts mit der begrifflichen oder ideellen Annäherung an die Bedeutung von Wörtern zu tun. Eine der Bedeutungen von _Nacht_ ist die Kollokationsfähigkeit mit _dunkel_, und von _dunkel_ natürlich die Kollokation mit _Nacht_.

Also die Beudeutung kommt nur durch Wörtern. Mittels matematischer Verfahren wird ermittelt, welche Wörter in ähnlichen Kontexten vorkommt. Am Ende hat man ein Modell bei dem man zu jedem Wort 500 Werte von 0 bis 1 hat. Details werden hier nicht erklärt.

// ## Word Embeddings


// Ein Verfahren in kürze. Es wird gesehe, welche Wörter mit welchen anderen Wörtern in gleichem Text vorkommt. Anhand des Musters kann man z. B. Synomme erkennen. 

// Und auch Haus und Wohnunge.

// Daraus kann man sagen, um zu erkluaren wie ähnlich sich wörter sind. Hier ein Beispiel (genr emitmachen)

// Es gibt unterschiedliche Verfahren. Ein einfaches: Man zählt wie oft welche andere Wörter bei einer Festengrößer Vorkommen. z. B.

// Dann stellt man eine Tabelle auf und kann sehen, welche Wörter wie oft vorkommen. Dass kann man bentuzen mut eine sog. Einbettung aufzustellen.

// Jeder Worter werden 500 Werte von 0-1 zugeweiesen. Damit kann dann lustige Sachen machen. Zum Beispiel kann man Wörter ansehen, die in ähnlcihen Kontexten vorkommen. Das können Synomme sein oder auch irgendwie verwandte Wörter.

## Der Bias (Probleme)

 Die aktuellen Verfahren sind jedoch unausgegoren. So können sie leicht „ausgetrickst“ werden oder sind zum Teil rassistisch

Ein Problem ist, dass bei großer Textmenge dann auch zum Bias kommen kann. So hat ein Forscher herausgefunden, dass eine Google-API, die zu ähnliches bentuzen. Das Word Mexiso negativ eingeordetnet. Das kam dadurch dass auf Hasskommentaren traniert wurd. Und es gab viel mehr negative Kommentem zu Mexio als zu anderern Ländern. 

## Mitmachen

Um besser zu verstehen um was es geht, haben wir selbst ein Embedding traniert. Auf XXX kommentaren von Online-Zeitungen. Probieren Sie es aus um es besser zu verstehen.

## Bitte ein Wort auswählen

[Radio value:q options:`['merkel', 'mittelmeer', 'miete', 'afd', 'migrant', 'überwachung']` /]

oder eigenes eingeben:

[var name:"q" value:"merkel" /]
[TextInput value:q  /]

[var name:"chooseQ" value:`{tokens:[]}` /]
[DataLoader value:chooseQ src:`"https://ptf-vecs.app.vis.one/typeahead_videos/german_comments_2010_2019_100k?q=" + q + "&n=100"` timeout:500/]

// cut of first token if it's the same

[Radio value:q options:`q === '' ? [] : (chooseQ.tokens[0] == q ? chooseQ.tokens.slice(1) : chooseQ.tokens)` /]

[var name:"qTable" value:`{tokens:[], sims: []}` /]
[DataLoader value:qTable src:`"https://ptf-vecs.app.vis.one/sim/german_comments_2010_2019_100k?q=" + (chooseQ.tokens[0] || q) + "&n=10"` timeout:1000 /]

[br /]

Die 10 ähnlichsten Wörter zu [Display value:`chooseQ.tokens[0]`/]:

[Table
    data:`qTable.tokens.map((k, i) => ({token: k, sim: qTable.sims[i]}));`
    columns:`[
        {
            Header: 'Wort',
            accessor: 'token',
        },{
            Header: 'Ähnlichkeit',
            accessor: 'sim',
            Cell: props => (props.row.sim * 100).toFixed(2) + "%",
            getProps: (state, rowInfo, column) => {
                return {
                    style: {
                        background: window.chroma.scale(['white', [42,42,123]])(rowInfo.row.sim),
                        color:  rowInfo.row.sim > 0.4 ? 'white': 'black'
                    },
                };
            },
        }
    ]`
/]

## Ausschnitte

Doch kann man nicht nur ein Wort und die Beziehuungen zu andern darstellen. Sondern auch die Beziehungen untereinandern. Die Achsen haben keine Bedeutung. Doch je ähnlciher sich Begriffe, desto näher sind sie zusammen. Die Achsen haben kein Bedeutung.

[var name:"nearest_small" value:`{data: [], tokens:[]}` /]
[DataLoader value:nearest_small small:true src:`"https://ptf-vecs.app.vis.one/nearest/german_comments_2010_2019_100k?q=" + (chooseQ.tokens[0] || q) + "&n="` timeout:2000/]

[var name:"scrollerIndex" value:0 /]

[Scroller currentStep:scrollerIndex]
  [Graphic]
      [Chart type:`"scatter"` data:`nearest_small.data.slice(0, 1 + scrollerIndex * (nearest_small.data.length === 7 ? 2 : 3))` labels:`nearest_small.tokens.slice(0, 1 + scrollerIndex * (nearest_small.data.length === 7 ? 2 : 3))` theme:scatter-theme size:`() => Math.max(window.screen.width, window.innerWidth) < 768 ? 4: 2` /]
  [/Graphic]

  [Step]## die nähesten[/Step]
  [Step]## mehr[/Step]
  [Step]## noch mehr[/Step]
  [Step]## jetzt all[/Step]
[/Scroller]

[hr/]

Das war die Limiterung auf ein paar wenige. Hier ein größer Ausblick. (Auf Desktop mit Mauss drüber)

[hr/]

[var name:"nearest_large" value:`{data: [], tokens:[]}` /]
[DataLoader value:nearest_large src:`"https://ptf-vecs.app.vis.one/nearest/german_comments_2010_2019_100k?q=" + (chooseQ.tokens[0] || q) + "&n=100"` timeout:2500 /]

[Graphic fullWidth:true]
  [Chart type:`"scatter"` data:`nearest_large.data` size:`() => Math.max(window.screen.width, window.innerWidth) < 768 ? 4: 2` labels:`[nearest_large.tokens[0]]` theme:scatter-theme events:`[
      {
        target: "data",
        eventHandlers: {
          onMouseOver: () => {
            return [{
              target: "labels",
              mutation: (props) => {
                return {text: nearest_large.tokens[props.index]};
              }
            }];
          },
          onMouseOut: () => {
            return [{
              target: "labels",
              mutation: (props) => {
                return {text: props.index === 0 ? nearest_large.tokens[0] : null}
              }
            }];
          }
        }
      }
    ]`
/]
[/Graphic]

[hr/]


## Veränderung über die Zeit

Doch wir haben nicht nur ein Embedding trainiert, sondern insgesamt fünf. Somit können wir über sehen, wie sich die Embeddings über die Zeit verändert haben. Weiter geht es mit den Änderungen in Teil II. Hier am Beispiel von [Display value:q/].

[hr/]


[Video q:q fullWidth:true/]

[hr /]

## Übersicht

* **[Teil I: Einführung](/)**
* [Teil II: Veränderung in den Kommentaren](/zeit/)
* [Teil III: Hintergrund zu Daten & Verfahren](/hintergrund/)

[hr /]

[Thanks/]