[meta title:"Wie Maschinen Kommentare verstehen – Kommentare.vis.one" description:"Maschinelles Lernen wird als Lösung verkauft, um Hass aus dem Internet zu filtern. Diese Webseite erklärt wie 'die Maschinen' die Bedeutung von Wörtern erlernen." url:"https://kommentare.vis.one/" twitterHandle:"@fil_ter" shareImageUrl:"https://kommentare.vis.one/static/images/preview.png" /]


[FilterHeader
  subtitle:"Maschinelles Lernen wird als Lösung verkauft, um Hass aus dem Internet zu filtern. Diese Webseite erklärt wie 'die Maschinen' die Bedeutung von Wörtern erlernen."
  fullWidth:true
  author:"Johannes Filter"
  authorLink:"https://johannesfilter.com"
  date:"15. März 2020"
  background:"#222222"
  color:"#ffffff"
/]

[MovingTexts /]

// https://github.com/FormidableLabs/victory/blob/master/packages/victory-core/src/victory-theme/grayscale.js

[derived name:'scatter-theme' value:`{
  axis: {
    style: {
      axis: {
        fill: 'transparent',
        stroke: 'transparent'
      },
      grid: {
        stroke: 'transparent',
      },
      tickLabels: {
        stroke: 'transparent',
        fill: 'transparent'
      }
    }
  },
  scatter: {
    style: {
      data: {
        fill: "black",
        cursor: 'pointer',
        opacity: 0.8
      },
      labels: {
        fontFamily: 'lato',
        fontSize: () => Math.max(window.screen.width, window.innerWidth) < 768 ? 20 :  8,
        letterSpacing: 'normal',
        padding: 3,
        fill: 'charcoal',
        stroke: "transparent"
      }
    }
  }
}` /]

In den Kommentarspalten tobt der Hass. Um diesen einzudämmen, prüfen Zeitungsredaktionen manuell die Kommentare. Doch aufgrund finanzieller Engpässe, schließen viele Zeitungen die Kommentarfunktion gleich ganz ab. In diesem Kontext gelobt die automatisierte Kommentareauswertung Besserung. Jedoch ist aktuell noch unklar, welche Aufgaben wir den Maschinen überlassen wollen. Damit es darüber eine gesellschaftliche Debatte gibt, müssen zuerst breite Teile der Bevölkerung die dahinter liegenden technischen Mechanismen verstehen. Mit dieser Webseite werden einzelne Verfahren des Machinelles Lernens am Beispiel von Kommentaren erklärt. Der Fokus liegt auf der Verarbeitung von Sprachen, Computerlinguistik (oder engl. *Natural-Language Processing*) funktioniert.

![](/static/images/news.png)
*[CC-BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/), [Christoph Hoppenbrock](https://bildbauer.de/), verändert ([Original](https://demokratielabore.de/workshops/newsbusters/))*

###### ignore please

## Was ist Maschinelles Lernen?

Mit dem Aufkommen der Informationstechnolgie im 20. Jahrhundert wurden immer mehr Entscheidungen an Computer abgegeben. Aufgaben wurden mithilfe von Programm-Code gelöst. Programmier:innen formulieren einen sogennanten Algorithmus, welche eine Schritt-für-Schritt-Rezept darstellt. Ähnlich zu einem Kochrezept werden Schritte festgelegt. Wie könnte ein Algorithmus aussehen, der Hasskommentare finden sollte? Ein Ansatz:

1. Man schreibt eine lange Liste von Beleidigungen auf.
2. Wenn eine Beleidigung in einem Kommentar vorkommt, wird der Kommentar aussortiert.

Nur lässt sich der Ansatz leicht austricksen, in dem man immer neue Wörter kreiert. Außderm würde die Länge der Liste ausufern. 

In diesem Zusammenhang gibt es eine neue Art Probleme zu Lösungen. Man füttert den Computer mit Daten und der Computer soll selbst lernen, welche Wörter als Beleidung zählt. Ansätze dieser Art nennt man Maschinelles Lernen (engl. *Machine Learning*). Es gibt zwei Arten von Maschine Learning: 
1. Trainingsdaten werden manuell annotiert und die Maschiene soll lernen von diesen zu extrapolieren ("überwachtes Lernen")
2. Es werden keine Daten extra annotiert, sondern von Information auf alten geschlossen werden. ("unüberwachtes Lernen")

Wir fokussieren auf den 2. Ansatz. Aus einer großen Menge an Text soll der Computer Wörter erlernen. Wir sehen uns nicht an, wie Kommentare als Hass klassifiert wird. Es geht zunächst darum, zu verstehen, wie die Maschiene Wörter versteht.

![](/static/images/computer.png)
*[CC-BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/), [Christoph Hoppenbrock](https://bildbauer.de/), verändert ([Original](https://demokratielabore.de))*

###### ignore please

## Sprache, Computer und Representation

Für dem Computer gibt es nur Nullen oder Einzen. Entweder Strom fließt oder nicht. In den Betriebssytemen der Computer sind jedoch Abstraktionsmodelle eingebaut, sodass sich kein Entwickler mit dem Fließen von Strom beschäftigen muss. Wir gehen davon aus, dass Text vorliegt. Im ersten Schritt unterteilen wir den Text in Wörter.

Mit einem sogennanten Tokenizer wird der Text in  Wörter aufgeteilt. In einfachen Fällen, wird am Leerzeichen getrennt. Aber schon bei Satzzeichen wird es kompliziert. Ist ein Punkt nun ein Satzende (und damit das vorherigen Buchstaben ein Wort) oder teile einer Abkürzung (und damit nicht zwangsläufig ein eigenes Wort)? Die Schwierigkeit erkennt man an diesem Beispiel.

[SVG src:"/static/images/tokenizer.svg" /]
*Text wird durch einen Tokenizer in Wörter aufgeteilt*

###### ignore please

Jetzt haben wir Wörter. Im nächsten Abschnitt führen wir uns, wie genau die Bedeutung entsteht.

## Bedeutung durch Kontext

Die Grundlegende Ansatz: Die Kolokation von Wörtern. Also welche Wörter werden mit welchen anderen Wörter im gleichen Satz auf. Der englische Linguister [John Rupert Firth](https://en.wikipedia.org/wiki/John_Rupert_Firth) stellt Mitte des 20. Jahrhunerts einige Theoreme auf.

[Aside]
> You shall know a word by the company it keeps. 
[/Aside]

> Du sollst ein Wort von der Begleitung kennen, die sie führt. 

[Aside]
> The complete meaning of a word is always contextual, and no study of meaning apart from context can be taken seriously. 
[/Aside]

> Die vollständige Bedeutung eines Wortes ist immer kontextabhängig, und keine Studie über die Bedeutung außer dem Kontext kann ernst genommen werden. 

Der Kontext entscheidet über die wahre Beudeutung eines Wortes. Und ein Wort kann durch 

[Aside]
> Meaning by collocation is an abstraction at the syntagmatic level and is not directly concerned with the conceptual or idea approach to the meaning of words. One of the meanings of _night_ is its collocability with _dark_, and of _dark_, of course, collocation with _night_.
[/Aside]

> Die Bedeutung durch Kollokation ist eine Abstraktion auf der syntagmatischen Ebene und hat nichts mit der begrifflichen oder ideellen Annäherung an die Bedeutung von Wörtern zu tun. Eine der Bedeutungen von _Nacht_ ist die Kollokationsfähigkeit mit _dunkel_, und von _dunkel_ natürlich die Kollokation mit _Nacht_.

Konrekt kann man das den Beispielen verstehen.

> Ich esse gerne **Fisch**. 

> Ich esse gerne **Bananen**.

> Ich esse gerne **Brot**.

Die drei Wörter kommen in ähnlichen Kontexten vor und daher weiß die Maschiene, dass die beiden Wörter in einem gewissen Maaße ähnlich sind. In diesem Fall, geht es um Lebensmitteln. Und so erlernt die Maschiene die Wörter.

[SVG src:"/static/images/window.svg" /]
*Über jedes Wort wird iteriert*

###### ignore please



Also die Beudeutung kommt nur durch Wörtern. Mittels matematischer Verfahren wird ermittelt, welche Wörter in ähnlichen Kontexten vorkommt. Am Ende hat man ein Modell bei dem man zu jedem Wort 500 Werte von 0 bis 1 hat. Details werden hier nicht erklärt. Nur so viel: Es wird ein Matrix aufgestellt, in ertem Schritt gezählt.


## Wie gut kann die Machine Beudeutungen erlenen? Ausprobieren!

Auf Grundlage von über 13 Millionen deutscher Online-Kommentare, gibt es nachfolgend die Möglichkeit die Macht von Word Embeddings auszuprobieren.

## Bitte ein Wort auswählen

[Radio value:q options:`['merkel', 'mittelmeer', 'mieten', 'afd', 'migrant', 'überwachung']` /]

oder eigenes Wort eingeben:

[var name:"q" value:"merkel" /]
[TextInput value:q  /]

[var name:"chooseQ" value:`{tokens:[]}` /]
[DataLoader value:chooseQ src:`"https://ptf-vecs.app.vis.one/typeahead_videos/german_comments_2010_2019_100k?q=" + q + "&n=100"` timeout:500/]

// cut of first token if it's the same

[Radio value:q options:`q === '' ? [] : (chooseQ.tokens[0] == q ? chooseQ.tokens.slice(1) : chooseQ.tokens)` /]

[var name:"qTable" value:`{tokens:[], sims: []}` /]
[DataLoader value:qTable src:`"https://ptf-vecs.app.vis.one/sim/german_comments_2010_2019_100k?q=" + (chooseQ.tokens[0] || q) + "&n=10"` timeout:1000 /]

[br /]

Die 10 ähnlichsten Wörter zu [Display value:`(chooseQ.tokens[0] ||  q)`/]:

[Table
    data:`qTable.tokens.map((k, i) => ({token: k, sim: qTable.sims[i]}));`
    columns:`[
        {
            Header: 'Wort',
            accessor: 'token',
        },{
            Header: 'Ähnlichkeit',
            accessor: 'sim',
            Cell: props => (props.row.sim * 100).toFixed(2) + "%",
            getProps: (state, rowInfo, column) => {
                return {
                    style: {
                        background: window.chroma.scale(['white', [42,42,123]])(rowInfo.row.sim),
                        color:  rowInfo.row.sim > 0.4 ? 'white': 'black'
                    },
                };
            },
        }
    ]`
/]

## Strukturen abbilden

Die ähnlichsten Begriffe als Rank darzustellen, ist ein einfacher Einstieg. Es werden jedoch die Strukturen zwischen den Begriffen verschleiert. Eine Projektion auf die Ebene bietet hier besseren Einblick. Hier ist zu beachten, dass Begriffe desto ähnlicher sich die Begriffe sind, desto näher sind sie zusammen. Die Achsen haben hier keine Bedeutung. [Hier erklären wir, wie genau es eine Ebene abgebildet werden kann.](/hintergrund/#todo)

[var name:"nearest_small" value:`{data: [], tokens:[]}` /]
[DataLoader value:nearest_small small:true src:`"https://ptf-vecs.app.vis.one/nearest/german_comments_2010_2019_100k?q=" + (chooseQ.tokens[0] || q) + "&n="` timeout:2000/]

[var name:"scrollerIndex" value:0 /]

[Scroller currentStep:scrollerIndex]
  [Graphic]
      [Chart type:`"scatter"` data:`nearest_small.data.slice(0, 1 + scrollerIndex * (nearest_small.data.length === 7 ? 2 : 3))` labels:`nearest_small.tokens.slice(0, 1 + scrollerIndex * (nearest_small.data.length === 7 ? 2 : 3))` theme:scatter-theme size:`() => Math.max(window.screen.width, window.innerWidth) < 768 ? 4: 2` /]
  [/Graphic]

  [Step]#### nur der Fokus [/Step]
  [Step]#### die nähesten Wörter[/Step]
  [Step]#### mehr Wörter[/Step]
  [Step]#### noch mehr Wörter[/Step]
[/Scroller]

[hr/]

Das ist nur ein Auschhnitt. In der Folge ein größere Ausschnitt. [Oder ein anderes Wort auswählen](#bitte-ein-wort-auswählen)



[hr/]

[var name:"nearest_large" value:`{data: [], tokens:[]}` /]
[DataLoader value:nearest_large src:`"https://ptf-vecs.app.vis.one/nearest/german_comments_2010_2019_100k?q=" + (chooseQ.tokens[0] || q) + "&n=100"` timeout:2500 /]

[Graphic fullWidth:true]
  [Chart type:`"scatter"` data:`nearest_large.data` size:`() => Math.max(window.screen.width, window.innerWidth) < 768 ? 4: 2` labels:`[nearest_large.tokens[0]]` theme:scatter-theme events:`[
      {
        target: "data",
        eventHandlers: {
          onMouseOver: () => {
            return [{
              target: "labels",
              mutation: (props) => {
                return {text: nearest_large.tokens[props.index]};
              }
            }];
          },
          onMouseOut: () => {
            return [{
              target: "labels",
              mutation: (props) => {
                return {text: props.index === 0 ? nearest_large.tokens[0] : null}
              }
            }];
          }
        }
      }
    ]`
/]
[/Graphic]

[Conditional if:`nearest_small.data.length !== 7`]
  Mitte der Maus über die Punkte fahren, um die Wörter zu sehen. [Oder ein anderes Wort auswählen](#bitte-ein-wort-auswählen)
[/Conditional]

[Conditional if:`nearest_small.data.length === 7`]
  [Ein anderes Wort auswählen](#bitte-ein-wort-auswählen)
[/Conditional]

[hr/]


## Veränderung über die Zeit

Im weiteren Verlauf haben wir uns angesehen, wie sich die Sprache verändert hat.

[👉 Weiter mit Teil II](/zeit/)

[hr /]

## Übersicht

* **[Teil I: Einführung](/)**
* [Teil II: Veränderung in den Kommentaren](/zeit/)
* [Teil III: Hintergrund zu Daten & Verfahren](/hintergrund/)

[hr /]

[Thanks/]