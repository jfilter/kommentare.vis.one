[meta title:"Wie Maschinen Kommentare verstehen ‚Äì Kommentare.vis.one" description:"Maschinelles Lernen wird als L√∂sung verkauft, um Hass aus dem Internet zu filtern. Diese Webseite erkl√§rt wie 'die Maschinen' die Bedeutung von W√∂rtern erlernen." url:"https://kommentare.vis.one/" twitterHandle:"@fil_ter" shareImageUrl:"https://kommentare.vis.one/static/images/preview.png" /]


[FilterHeader
  subtitle:"Maschinelles Lernen wird als L√∂sung verkauft, um Hass aus dem Internet zu filtern. Diese Webseite erkl√§rt wie 'die Maschinen' die Bedeutung von W√∂rtern erlernen."
  fullWidth:true
  author:"Johannes Filter"
  authorLink:"https://johannesfilter.com"
  date:"15. M√§rz 2020"
  background:"#222222"
  color:"#ffffff"
/]

[MovingTexts /]

// https://github.com/FormidableLabs/victory/blob/master/packages/victory-core/src/victory-theme/grayscale.js

[derived name:'scatter-theme' value:`{
  axis: {
    style: {
      axis: {
        fill: 'transparent',
        stroke: 'transparent'
      },
      grid: {
        stroke: 'transparent',
      },
      tickLabels: {
        stroke: 'transparent',
        fill: 'transparent'
      }
    }
  },
  scatter: {
    style: {
      data: {
        fill: "black",
        cursor: 'pointer',
        opacity: 0.8
      },
      labels: {
        fontFamily: 'lato',
        fontSize: () => Math.max(window.screen.width, window.innerWidth) < 768 ? 20 :  8,
        letterSpacing: 'normal',
        padding: 3,
        fill: 'charcoal',
        stroke: "transparent"
      }
    }
  }
}` /]

In den Kommentarspalten tobt der Hass. Um diesen einzud√§mmen, pr√ºfen Zeitungsredaktionen manuell die Kommentare. Doch aufgrund finanzieller Engp√§sse, schlie√üen viele Zeitungen die Kommentarfunktion gleich ganz ab. In diesem Kontext gelobt die automatisierte Kommentareauswertung Besserung. Jedoch ist aktuell noch unklar, welche Aufgaben wir den Maschinen √ºberlassen wollen. Damit es dar√ºber eine gesellschaftliche Debatte gibt, m√ºssen zuerst breite Teile der Bev√∂lkerung die dahinter liegenden technischen Mechanismen verstehen. Mit dieser Webseite werden einzelne Verfahren des Machinelles Lernens am Beispiel von Kommentaren erkl√§rt. Der Fokus liegt auf der Verarbeitung von Sprachen, Computerlinguistik (oder engl. *Natural-Language Processing*) funktioniert.

![](/static/images/news.png)
*[CC-BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/), [Christoph Hoppenbrock](https://bildbauer.de/), ver√§ndert ([Original](https://demokratielabore.de/workshops/newsbusters/))*

###### ignore please

## Was ist Maschinelles Lernen?

Mit dem Aufkommen der Informationstechnolgie im 20. Jahrhundert wurden immer mehr Entscheidungen an Computer abgegeben. Aufgaben wurden mithilfe von Programm-Code gel√∂st. Programmier:innen formulieren einen sogennanten Algorithmus, welche eine Schritt-f√ºr-Schritt-Rezept darstellt. √Ñhnlich zu einem Kochrezept werden Schritte festgelegt. Wie k√∂nnte ein Algorithmus aussehen, der Hasskommentare finden sollte? Ein Ansatz:

1. Man schreibt eine lange Liste von Beleidigungen auf.
2. Wenn eine Beleidigung in einem Kommentar vorkommt, wird der Kommentar aussortiert.

Nur l√§sst sich der Ansatz leicht austricksen, in dem man immer neue W√∂rter kreiert. Au√üderm w√ºrde die L√§nge der Liste ausufern. 

In diesem Zusammenhang gibt es eine neue Art Probleme zu L√∂sungen. Man f√ºttert den Computer mit Daten und der Computer soll selbst lernen, welche W√∂rter als Beleidung z√§hlt. Ans√§tze dieser Art nennt man Maschinelles Lernen (engl. *Machine Learning*). Es gibt zwei Arten von Maschine Learning: 
1. Trainingsdaten werden manuell annotiert und die Maschiene soll lernen von diesen zu extrapolieren ("√ºberwachtes Lernen")
2. Es werden keine Daten extra annotiert, sondern von Information auf alten geschlossen werden. ("un√ºberwachtes Lernen")

Wir fokussieren auf den 2. Ansatz. Aus einer gro√üen Menge an Text soll der Computer W√∂rter erlernen. Wir sehen uns nicht an, wie Kommentare als Hass klassifiert wird. Es geht zun√§chst darum, zu verstehen, wie die Maschiene W√∂rter versteht.

![](/static/images/computer.png)
*[CC-BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/), [Christoph Hoppenbrock](https://bildbauer.de/), ver√§ndert ([Original](https://demokratielabore.de))*

###### ignore please

## Sprache, Computer und Representation

F√ºr dem Computer gibt es nur Nullen oder Einzen. Entweder Strom flie√üt oder nicht. In den Betriebssytemen der Computer sind jedoch Abstraktionsmodelle eingebaut, sodass sich kein Entwickler mit dem Flie√üen von Strom besch√§ftigen muss. Wir gehen davon aus, dass Text vorliegt. Im ersten Schritt unterteilen wir den Text in W√∂rter.

Mit einem sogennanten Tokenizer wird der Text in  W√∂rter aufgeteilt. In einfachen F√§llen, wird am Leerzeichen getrennt. Aber schon bei Satzzeichen wird es kompliziert. Ist ein Punkt nun ein Satzende (und damit das vorherigen Buchstaben ein Wort) oder teile einer Abk√ºrzung (und damit nicht zwangsl√§ufig ein eigenes Wort)? Die Schwierigkeit erkennt man an diesem Beispiel.

[SVG src:"/static/images/tokenizer.svg" /]
*Text wird durch einen Tokenizer in W√∂rter aufgeteilt*

###### ignore please

Jetzt haben wir W√∂rter. Im n√§chsten Abschnitt f√ºhren wir uns, wie genau die Bedeutung entsteht.

## Bedeutung durch Kontext

Die Grundlegende Ansatz: Die Kolokation von W√∂rtern. Also welche W√∂rter werden mit welchen anderen W√∂rter im gleichen Satz auf. Der englische Linguister [John Rupert Firth](https://en.wikipedia.org/wiki/John_Rupert_Firth) stellt Mitte des 20. Jahrhunerts einige Theoreme auf.

[Aside]
> You shall know a word by the company it keeps. 
[/Aside]

> Du sollst ein Wort von der Begleitung kennen, die sie f√ºhrt. 

[Aside]
> The complete meaning of a word is always contextual, and no study of meaning apart from context can be taken seriously. 
[/Aside]

> Die vollst√§ndige Bedeutung eines Wortes ist immer kontextabh√§ngig, und keine Studie √ºber die Bedeutung au√üer dem Kontext kann ernst genommen werden. 

Der Kontext entscheidet √ºber die wahre Beudeutung eines Wortes. Und ein Wort kann durch 

[Aside]
> Meaning by collocation is an abstraction at the syntagmatic level and is not directly concerned with the conceptual or idea approach to the meaning of words. One of the meanings of _night_ is its collocability with _dark_, and of _dark_, of course, collocation with _night_.
[/Aside]

> Die Bedeutung durch Kollokation ist eine Abstraktion auf der syntagmatischen Ebene und hat nichts mit der begrifflichen oder ideellen Ann√§herung an die Bedeutung von W√∂rtern zu tun. Eine der Bedeutungen von _Nacht_ ist die Kollokationsf√§higkeit mit _dunkel_, und von _dunkel_ nat√ºrlich die Kollokation mit _Nacht_.

Konrekt kann man das den Beispielen verstehen.

> Ich esse gerne **Fisch**. 

> Ich esse gerne **Bananen**.

> Ich esse gerne **Brot**.

Die drei W√∂rter kommen in √§hnlichen Kontexten vor und daher wei√ü die Maschiene, dass die beiden W√∂rter in einem gewissen Maa√üe √§hnlich sind. In diesem Fall, geht es um Lebensmitteln. Und so erlernt die Maschiene die W√∂rter.

[SVG src:"/static/images/window.svg" /]
*√úber jedes Wort wird iteriert*

###### ignore please



Also die Beudeutung kommt nur durch W√∂rtern. Mittels matematischer Verfahren wird ermittelt, welche W√∂rter in √§hnlichen Kontexten vorkommt. Am Ende hat man ein Modell bei dem man zu jedem Wort 500 Werte von 0 bis 1 hat. Details werden hier nicht erkl√§rt. Nur so viel: Es wird ein Matrix aufgestellt, in ertem Schritt gez√§hlt.


## Wie gut kann die Machine Beudeutungen erlenen? Ausprobieren!

Auf Grundlage von √ºber 13 Millionen deutscher Online-Kommentare, gibt es nachfolgend die M√∂glichkeit die Macht von Word Embeddings auszuprobieren.

## Bitte ein Wort ausw√§hlen

[Radio value:q options:`['merkel', 'mittelmeer', 'mieten', 'afd', 'migrant', '√ºberwachung']` /]

oder eigenes Wort eingeben:

[var name:"q" value:"merkel" /]
[TextInput value:q  /]

[var name:"chooseQ" value:`{tokens:[]}` /]
[DataLoader value:chooseQ src:`"https://ptf-vecs.app.vis.one/typeahead_videos/german_comments_2010_2019_100k?q=" + q + "&n=100"` timeout:500/]

// cut of first token if it's the same

[Radio value:q options:`q === '' ? [] : (chooseQ.tokens[0] == q ? chooseQ.tokens.slice(1) : chooseQ.tokens)` /]

[var name:"qTable" value:`{tokens:[], sims: []}` /]
[DataLoader value:qTable src:`"https://ptf-vecs.app.vis.one/sim/german_comments_2010_2019_100k?q=" + (chooseQ.tokens[0] || q) + "&n=10"` timeout:1000 /]

[br /]

Die 10 √§hnlichsten W√∂rter zu [Display value:`(chooseQ.tokens[0] ||  q)`/]:

[Table
    data:`qTable.tokens.map((k, i) => ({token: k, sim: qTable.sims[i]}));`
    columns:`[
        {
            Header: 'Wort',
            accessor: 'token',
        },{
            Header: '√Ñhnlichkeit',
            accessor: 'sim',
            Cell: props => (props.row.sim * 100).toFixed(2) + "%",
            getProps: (state, rowInfo, column) => {
                return {
                    style: {
                        background: window.chroma.scale(['white', [42,42,123]])(rowInfo.row.sim),
                        color:  rowInfo.row.sim > 0.4 ? 'white': 'black'
                    },
                };
            },
        }
    ]`
/]

## Strukturen abbilden

Die √§hnlichsten Begriffe als Rank darzustellen, ist ein einfacher Einstieg. Es werden jedoch die Strukturen zwischen den Begriffen verschleiert. Eine Projektion auf die Ebene bietet hier besseren Einblick. Hier ist zu beachten, dass Begriffe desto √§hnlicher sich die Begriffe sind, desto n√§her sind sie zusammen. Die Achsen haben hier keine Bedeutung. [Hier erkl√§ren wir, wie genau es eine Ebene abgebildet werden kann.](/hintergrund/#todo)

[var name:"nearest_small" value:`{data: [], tokens:[]}` /]
[DataLoader value:nearest_small small:true src:`"https://ptf-vecs.app.vis.one/nearest/german_comments_2010_2019_100k?q=" + (chooseQ.tokens[0] || q) + "&n="` timeout:2000/]

[var name:"scrollerIndex" value:0 /]

[Scroller currentStep:scrollerIndex]
  [Graphic]
      [Chart type:`"scatter"` data:`nearest_small.data.slice(0, 1 + scrollerIndex * (nearest_small.data.length === 7 ? 2 : 3))` labels:`nearest_small.tokens.slice(0, 1 + scrollerIndex * (nearest_small.data.length === 7 ? 2 : 3))` theme:scatter-theme size:`() => Math.max(window.screen.width, window.innerWidth) < 768 ? 4: 2` /]
  [/Graphic]

  [Step]#### nur der Fokus [/Step]
  [Step]#### die n√§hesten W√∂rter[/Step]
  [Step]#### mehr W√∂rter[/Step]
  [Step]#### noch mehr W√∂rter[/Step]
[/Scroller]

[hr/]

Das ist nur ein Auschhnitt. In der Folge ein gr√∂√üere Ausschnitt. [Oder ein anderes Wort ausw√§hlen](#bitte-ein-wort-ausw√§hlen)



[hr/]

[var name:"nearest_large" value:`{data: [], tokens:[]}` /]
[DataLoader value:nearest_large src:`"https://ptf-vecs.app.vis.one/nearest/german_comments_2010_2019_100k?q=" + (chooseQ.tokens[0] || q) + "&n=100"` timeout:2500 /]

[Graphic fullWidth:true]
  [Chart type:`"scatter"` data:`nearest_large.data` size:`() => Math.max(window.screen.width, window.innerWidth) < 768 ? 4: 2` labels:`[nearest_large.tokens[0]]` theme:scatter-theme events:`[
      {
        target: "data",
        eventHandlers: {
          onMouseOver: () => {
            return [{
              target: "labels",
              mutation: (props) => {
                return {text: nearest_large.tokens[props.index]};
              }
            }];
          },
          onMouseOut: () => {
            return [{
              target: "labels",
              mutation: (props) => {
                return {text: props.index === 0 ? nearest_large.tokens[0] : null}
              }
            }];
          }
        }
      }
    ]`
/]
[/Graphic]

[Conditional if:`nearest_small.data.length !== 7`]
  Mitte der Maus √ºber die Punkte fahren, um die W√∂rter zu sehen. [Oder ein anderes Wort ausw√§hlen](#bitte-ein-wort-ausw√§hlen)
[/Conditional]

[Conditional if:`nearest_small.data.length === 7`]
  [Ein anderes Wort ausw√§hlen](#bitte-ein-wort-ausw√§hlen)
[/Conditional]

[hr/]


## Ver√§nderung √ºber die Zeit

Im weiteren Verlauf haben wir uns angesehen, wie sich die Sprache ver√§ndert hat.

[üëâ Weiter mit Teil II](/zeit/)

[hr /]

## √úbersicht

* **[Teil I: Einf√ºhrung](/)**
* [Teil II: Ver√§nderung in den Kommentaren](/zeit/)
* [Teil III: Hintergrund zu Daten & Verfahren](/hintergrund/)

[hr /]

[Thanks/]