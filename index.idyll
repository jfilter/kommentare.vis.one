[meta title:"Kommentare" description:"NLP am Beispiel von Online-Kommentaren erklärt" /]


[FilterHeader
  subtitle:"Maschinelles Lernen wird als Lösung verkauft, um Hass aus dem Internet zu filtern. Diese Webseite erklärt interaktiv wie Computer die Bedeutung von Wörtern lernen."
  fullWidth:true
  author:"Johannes Filter"
  authorLink:"https://johannesfilter.com"
  date:"15. Februar 2020"
  background:"#222222"
  color:"#ffffff"
/]

[MovingTexts /]

// Zudem haben wir 15 Millionen Kommentare analysiert, wie sich die Sprache über die letzten zehn Jahre in der Kommentarspalte verändert hat.

// https://github.com/FormidableLabs/victory/blob/master/packages/victory-core/src/victory-theme/grayscale.js

[derived name:'scatter-theme' value:`{
  axis: {
    style: {
      axis: {
        fill: 'transparent',
        stroke: 'transparent'
      },
      grid: {
        stroke: 'transparent',
      },
      tickLabels: {
        stroke: 'transparent',
        fill: 'transparent'
      }
    }
  },
  scatter: {
    style: {
      data: {
        fill: "black",
        cursor: 'pointer',
        opacity: 0.8
      },
      labels: {
        fontFamily: 'lato',
        fontSize: () => Math.max(window.screen.width, window.innerWidth) < 768 ? 20 :  8,
        letterSpacing: 'normal',
        padding: 3,
        fill: 'charcoal',
        stroke: "transparent"
      }
    }
  }
}` /]

In den Kommentarspalten des Internet tobt der Hass. Um diesen zu einzudämmen, müssen Kommentare manuell geprüft werden. Weil dies viele Zeitungsredaktionen überfordert, schließen sie die Kommentarfunktion gleich ganz ab. In diesem Kontext gelobt Maschinelles Lernen (engl. *Machine Learning*) Besserung, indem automatisiert Kommentare ausgewertet werden.

Die aktuellen Verfahren sind jedoch unausgegoren. So können sie leicht „ausgetrickst“ werden oder sind zum Teil rassistisch. Zudem ist aktuell ist noch nicht klar, welche Aufgaben wir den Maschinen überlassen wollen. Damit es darüber eine gesellschaftliche Debatte gibt, müssen zuerst breite Teile der Bevölkerung die dahinter liegenden technischen Mechanismen verstehen. Mit dieser Webseite werden einzelne Verfahren der Computerlinguistik (oder Verarbeitung natürlicher Sprache (engl. *Natural-Language Processing* )) funktioniert.

![](/static/images/news.png)
*[CC-BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/), [Christoph Hoppenbrock](https://bildbauer.de/), verändert ([Original](https://demokratielabore.de/workshops/newsbusters/))*

###### ignore please

## Was ist Maschinelles Lernen?
Mit dem Aufkommen der Informationstechnolgie im 20. Jahrhunert wurden immer mehr Entscheidungen an Computer abgegeben. Aufgaben wurden mithilfe von Programm-Code gelöst. Programmieren formulieren Algorithmen als Schritt-für-Schritt Rezzept, welche man sich als Kochrezept vorstellen sollte. Wie würde das am Beispiel Hass-Kommentaren aussehen? Ein Ansatz: Man schreibt eine lange Liste von Beleidigungen auf. Wenn eine Beleidigung in einem Kommentar vorkommt, wird dieser gelöscht. Nur lässt sich der Ansatz leicht austricksen, in dem man immer neue Wörter kreiert. Außderm würde die Länge der Liste ausuffern.

Mithilfe von Maschinellem Lernen, soll der Computer anhand von Trainingsdaten lernen. Es gibt auch hier untersschiedliche Verfahren. Wir fokusieren uns auf einen


## Sprache und Computer, Represeation

Auf der untersten Ebene werden Daten als 0 oder 1 abgespeichert. Es gibt jedoch Abstraktionseben die darauf aufbauen. Aus 0 und 1 werden Zeichen. Und aus Zeichen werde Buchstaben. Mit Buchstaben kann man Wörter und damit ganze Texte bauen. Für unseren Fall wollen wir einen Text als eine Liste von Wörtern betrachten. Dafür muss im ersten Schritt aus einem Text wöter bauen. Wie kommen wir dann von Wörtern auf Bedeutung? 

![](/static/images/computer.png)
*[CC-BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/), [Christoph Hoppenbrock](https://bildbauer.de/), verändert ([Original](https://demokratielabore.de))*

###### bla


## Beudeutung durch Kontext

Wir sehen uns an, welchen unterschiedlichen Kontexten Wörter vorkommt. Dabei beruft sich die Computerlinguistik auf Theomreke von Liniguisten aus den 50er Jahren. Wie kommt man von Wörtern zu Kontext?

Der englische Linguister [John Rupert Firth](https://en.wikipedia.org/wiki/John_Rupert_Firth) stellt Mitte des 20. Jahrhunerts einige Theoreme auf.

[Aside]
> You shall know a word by the company it keeps. 
[/Aside]

> Du sollst ein Wort von der Begleitung kennen, die sie führt. 

[Aside]
> The complete meaning of a word is always contextual, and no study of meaning apart from context can be taken seriously. 
[/Aside]

> The complete meaning of a word is always contextual, and no study of meaning apart from context can be taken seriously. 

Der Kontext entscheidet über die wahre Beudeutung eines Wortes. Und ein Wort kann durch 

[Aside]
> Meaning by collocation is an abstraction at the syntagmatic level and is not directly concerned with the conceptual or idea approach to the meaning of words. One of the meanings of _night_ is its collocability with _dark_, and of _dark_, of course, collocation with _night_.
[/Aside]

> Meaning by collocation is an abstraction at the syntagmatic level and is not directly concerned with the conceptual or idea approach to the meaning of words. One of the meanings of _night_ is its collocability with _dark_, and of _dark_, of course, collocation with _night_.

Die Kollokation kommt einerm besondern Gewicht hinzu.

Diese Hyphoesen liegen einive Verahren aus dem Berepch der automatiserten Sprachverarbeitung (Natural Language Processing)

## Word Embeddings

Ein Verfahren in kürze. Es wird gesehe, welche Wörter mit welchen anderen Wörtern in gleichem Text vorkommt. Anhand des Musters kann man z. B. Synomme erkennen. 

Und auch Haus und Wohnunge.

Daraus kann man sagen, um zu erkluaren wie ähnlich sich wörter sind. Hier ein Beispiel (genr emitmachen)

Es gibt unterschiedliche Verfahren. Ein einfaches: Man zählt wie oft welche andere Wörter bei einer Festengrößer Vorkommen. z. B.

Dann stellt man eine Tabelle auf und kann sehen, welche Wörter wie oft vorkommen. Dass kann man bentuzen mut eine sog. Einbettung aufzustellen.

Jeder Worter werden 500 Werte von 0-1 zugeweiesen. Damit kann dann lustige Sachen machen. Zum Beispiel kann man Wörter ansehen, die in ähnlcihen Kontexten vorkommen. Das können Synomme sein oder auch irgendwie verwandte Wörter.

## Probleme

Ein Problem ist, dass bei großer Textmenge dann auch zum Bias kommen kann. So hat ein Forscher herausgefunden, dass eine Google-API, die zu ähnliches bentuzen. Das Word Mexiso negativ eingeordetnet. Das kam dadurch dass auf Hasskommentaren traniert wurd. Und es gab viel mehr negative Kommentem zu Mexio als zu anderern Ländern. 

## Auswahl

[Radio value:q options:`['merkel', 'mittelmeer', 'mieten', 'diesel', 'manager']` /]


[var name:"q" value:"merkel" /]
[TextInput value:q /]

[var name:"chooseQ" value:`{tokens:[]}` /]
[DataLoader value:chooseQ src:`"https://ptf-vecs.app.vis.one/typeahead/german_comments_2010_2019_100k?q=" + q + "&n=100"` timeout:500/]

// cut of first token if it's the same

[Radio value:q options:`q === '' ? [] : (chooseQ.tokens[0] == q ? chooseQ.tokens.slice(1) : chooseQ.tokens)` /]

[var name:"qTable" value:`{tokens:[], sims: []}` /]
[DataLoader value:qTable src:`"https://ptf-vecs.app.vis.one/sim/german_comments_2010_2019_100k?q=" + (chooseQ.tokens[0] || q) + "&n=10"` timeout:1000 /]

[br /]

Die 10 ähnlichsten Wörter zu [Display value:`chooseQ.tokens[0]`/]:

[Table
    data:`qTable.tokens.map((k, i) => ({token: k, sim: qTable.sims[i]}));`
    columns:`[
        {
            Header: 'Wort',
            accessor: 'token',
        },{
            Header: 'Ähnlichkeit',
            accessor: 'sim',
            Cell: props => (props.row.sim * 100).toFixed(2) + "%",
            getProps: (state, rowInfo, column) => {
                return {
                    style: {
                        background: window.chroma.scale(['white', [42,42,123]])(rowInfo.row.sim),
                        color:  rowInfo.row.sim > 0.4 ? 'white': 'black'
                    },
                };
            },
        }
    ]`
/]

## Ausschnitte

Doch kann man nicht nur ein Wort und die Beziehuungen zu andern darstellen. Sondern auch die Beziehungen untereinandern. Die Achsen haben keine Bedeutung. Doch je ähnlciher sich Begriffe

[var name:"nearest_small" value:`{data: [], tokens:[]}` /]
[DataLoader value:nearest_small small:true src:`"https://ptf-vecs.app.vis.one/nearest/german_comments_2010_2019_100k?q=" + (chooseQ.tokens[0] || q) + "&n="` timeout:2000/]

[var name:"scrollerIndex" value:0 /]


[Scroller currentStep:scrollerIndex]
  [Graphic]
      [Chart type:`"scatter"` data:`nearest_small.data.slice(0, 1 + scrollerIndex * (nearest_small.data.length === 7 ? 2 : 3))` labels:`nearest_small.tokens.slice(0, 1 + scrollerIndex * (nearest_small.data.length === 7 ? 2 : 3))` theme:scatter-theme size:`() => Math.max(window.screen.width, window.innerWidth) < 768 ? 4: 2` /]
  [/Graphic]

  [Step]## Hey, neat, here's some data![/Step]
  [Step]## ...   [/Step]
  [Step]## ...  What's that more data?![/Step]
  [Step]## ...  What's that more data?![/Step]

[/Scroller]

## HUGE

[var name:"nearest_large" value:`{data: [], tokens:[]}` /]
[DataLoader value:nearest_large src:`"https://ptf-vecs.app.vis.one/nearest/german_comments_2010_2019_100k?q=" + (chooseQ.tokens[0] || q) + "&n=100"` timeout:2500 /]

[Graphic fullWidth:true]
  [Chart type:`"scatter"` data:`nearest_large.data` size:`() => Math.max(window.screen.width, window.innerWidth) < 768 ? 4: 2` labels:`[nearest_large.tokens[0]]` theme:scatter-theme events:`[
      {
        target: "data",
        eventHandlers: {
          onMouseOver: () => {
            return [{
              target: "labels",
              mutation: (props) => {
                return {text: nearest_large.tokens[props.index]};
              }
            }];
          },
          onMouseOut: () => {
            return [{
              target: "labels",
              mutation: (props) => {
                return {text: props.index === 0 ? nearest_large.tokens[0] : null}
              }
            }];
          }
        }
      }
    ]`
/]
[/Graphic]


## Veränderung über die Zeit

Hier Teasern wir ein paar Beispiele.

[Video q:q fullWidth:true/]

[hr /]

## Übersicht

* **[Teil I: Einführung](./)**
* [Teil II: Veränderung über die Zeit](./zeit/)
* [Teil III: Hintergrund über Daten & Verfahren](./hintergrund/)
